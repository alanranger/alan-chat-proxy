// /api/chat.js
export const config = { runtime: 'nodejs' };

import { createClient } from '@supabase/supabase-js';

const need = (k) => {
  const v = process.env[k];
  if (!v || !v.trim()) throw new Error(`missing_env:${k}`);
  return v;
};
const opt = (k) => process.env[k] || '';

const toJSON = (res, status, obj) => {
  res.setHeader('Content-Type', 'application/json; charset=utf-8');
  res.status(status).send(JSON.stringify(obj));
};

// Simple helper to POST JSON
async function postJSON(url, body, headers = {}) {
  const r = await fetch(url, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json', ...headers },
    body: JSON.stringify(body),
  });
  if (!r.ok) {
    const text = await r.text().catch(() => '');
    throw new Error(`llm_http_${r.status}:${text.slice(0, 400)}`);
  }
  return r.json();
}

// Pick LLM provider (OpenRouter OR OpenAI), one source of truth is kept for embeddings already;
// here we just pick whichever key you provided.
function getLLMConfig() {
  const orKey = opt('OPENROUTER_API_KEY');
  const oaKey = opt('OPENAI_API_KEY');
  if (orKey) {
    // OpenRouter: pick a reliable GPT-4o-mini compatible model name
    return {
      provider: 'openrouter',
      endpoint: 'https://openrouter.ai/api/v1/chat/completions',
      headers: { Authorization: `Bearer ${orKey}` },
      model: 'openai/gpt-4o-mini',
    };
  }
  if (oaKey) {
    // OpenAI
    return {
      provider: 'openai',
      endpoint: 'https://api.openai.com/v1/chat/completions',
      headers: { Authorization: `Bearer ${oaKey}` },
      model: 'gpt-4o-mini',
    };
  }
  throw new Error('no_llm_provider_configured');
}

const SYS_PROMPT = `
You are "Alan Ranger Assistant", a helpful photography guide and course/workshop assistant.
You must answer succinctly, friendly, and practically—like a patient tutor. Always:
- Use clear headings or short paragraphs.
- Prefer bullets for steps, tips, gear lists.
- Include concrete next steps when helpful.
- NEVER hallucinate; if unsure, say so and suggest a relevant action.
- ONLY use the snippets provided as context—no outside knowledge beyond general photography basics.
- Cite sources from provided context by listing URL chips (do not inline superscripts).

OUTPUT STRICT JSON with the shape:
{
  "answer": "markdown-friendly text (no HTML, use **bold**, *italic*, - bullets, 1. numbered lists, small code blocks allowed)",
  "citations": ["https://...","https://..."],
  "followUps": ["short clickable suggestion", "…", "…"]
}

Do not include any extra keys or commentary. JSON only.
`.trim();

function buildUserPrompt(query, matches) {
  // Turn DB snippets into compact context blocks
  const lines = matches.map((m, i) => {
    const cleaned = (m.content || '')
      .replace(/\s+/g, ' ')
      .slice(0, 1400); // bound context per chunk
    return `# Source ${i + 1}
URL: ${m.url}
Text: ${cleaned}`;
  });

  return `
User question: ${query}

Context snippets:
${lines.join('\n\n')}
`.trim();
}

async function embedAndSearch(query, topK) {
  // Reuse your /api/search for the same embedding model to keep single source of truth.
  const r = await fetch(`${process.env.VERCEL_URL ? 'https://' + process.env.VERCEL_URL : ''}/api/search`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ query, topK }),
  });
  if (!r.ok) {
    const text = await r.text().catch(() => '');
    throw new Error(`search_failed_${r.status}:${text.slice(0, 300)}`);
  }
  const j = await r.json();
  if (!j || !Array.isArray(j.matches)) return { matches: [] };
  return j;
}

export default async function handler(req, res) {
  try {
    if (req.method !== 'POST') return toJSON(res, 405, { error: 'method_not_allowed' });

    const { query, topK = 8 } = req.body || {};
    if (!query || typeof query !== 'string') return toJSON(res, 400, { error: 'bad_request', detail: 'Provide "query" string.' });

    // 1) Retrieve snippets with your RAG search
    const { matches } = await embedAndSearch(query, Math.max(1, Math.min(16, +topK || 8)));

    // 2) Build LLM request with strict JSON output instruction
    const { provider, endpoint, headers, model } = getLLMConfig();

    const userPrompt = buildUserPrompt(query, matches);

    const body = {
      model,
      temperature: 0.2,
      messages: [
        { role: 'system', content: SYS_PROMPT },
        { role: 'user', content: userPrompt }
      ]
    };

    const j = await postJSON(endpoint, body, headers);

    // 3) Parse LLM JSON
    let raw = (j?.choices?.[0]?.message?.content || '').trim();
    // Some providers wrap JSON in ```json fences—strip if present
    raw = raw.replace(/^```json\s*|\s*```$/g, '');
    let out;
    try {
      out = JSON.parse(raw);
    } catch {
      // Last-resort fallback if the model returned a sentence—wrap it
      out = {
        answer: raw || "I couldn't produce an answer. Please try rephrasing your question.",
        citations: Array.from(new Set(matches.map(m => m.url))).slice(0, 5),
        followUps: ["Show workshops near me", "Beginner camera settings", "Tuition and mentoring options"]
      };
    }

    // 4) Ensure citations only include known URLs
    const knownURLs = new Set(matches.map(m => m.url));
    out.citations = (Array.isArray(out.citations) ? out.citations : [])
      .filter(u => knownURLs.has(u))
      .slice(0, 6);

    // 5) Sensible follow-ups if none
    if (!Array.isArray(out.followUps) || out.followUps.length === 0) {
      out.followUps = ["Workshops near Coventry", "Which lens for landscape?", "How to prepare for a workshop"];
    }

    return toJSON(res, 200, {
      ok: true,
      answer: out.answer || "I don't know.",
      citations: out.citations,
      followUps: out.followUps
    });
  } catch (err) {
    return toJSON(res, 500, { error: 'server_error', detail: err?.message || String(err) });
  }
}
